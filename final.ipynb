{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04eab8d6-5aca-4566-b2ef-16135a7d98fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b201d3bd-a0d0-4138-a96b-c861651ce2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model_dict = pickle.load(open('./model.p', 'rb'))\n",
    "model = model_dict['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a915ad3-448b-4657-b94d-f2eb89caeb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize video capture and MediaPipe Hands\n",
    "cap = cv2.VideoCapture(0)\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "hands = mp_hands.Hands(static_image_mode=False, min_detection_confidence=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95c25098-e994-4990-b2d5-f39da363d2f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n",
      "Feature capture started.\n",
      "Feature count: 42\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unrecognized data type: x=[0.24120488800549053, 1.0, 0.6937507124705689, 0.8990371985032555, 0.9057369185712403, 0.7056790022649976, 0.9467768652420852, 0.525807541471704, 1.0, 0.39026506625768903, 0.6736694802706051, 0.49409920192560297, 0.7824388143176209, 0.30520251256162473, 0.8302748506216027, 0.18640122764291142, 0.8517760982367321, 0.08245851646205951, 0.4501800703393166, 0.4683188782936545, 0.5275061723993476, 0.25533928220639646, 0.5631709830850691, 0.11651409884099338, 0.5807576887009087, 0.0, 0.22954898677889182, 0.47779349706639024, 0.2837783762660866, 0.2709084782387454, 0.34145061118833286, 0.1424759240456597, 0.3891468507979905, 0.034684551154154745, 0.0, 0.5197089702899528, 0.013250193393245756, 0.36608171978878956, 0.03225714898246302, 0.2606386251914124, 0.05730163294735998, 0.16211389127538064] (of type <class 'list'>)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 73\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# Ensure that model is loaded before making predictions\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model:\n\u001b[1;32m---> 73\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(data_aux)\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;66;03m# Convert the prediction to a scalar and map it to the corresponding character\u001b[39;00m\n\u001b[0;32m     76\u001b[0m     predicted_character \u001b[38;5;241m=\u001b[39m labels_dict[\u001b[38;5;28mint\u001b[39m(prediction\u001b[38;5;241m.\u001b[39mitem())]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\trainers\\data_adapters\\__init__.py:120\u001b[0m, in \u001b[0;36mget_data_adapter\u001b[1;34m(x, y, sample_weight, batch_size, steps_per_epoch, shuffle, class_weight)\u001b[0m\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m GeneratorDataAdapter(x)\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;66;03m# TODO: should we warn or not?\u001b[39;00m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;66;03m# warnings.warn(\u001b[39;00m\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;66;03m#     \"`shuffle=True` was passed, but will be ignored since the \"\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 120\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized data type: x=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(x)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Unrecognized data type: x=[0.24120488800549053, 1.0, 0.6937507124705689, 0.8990371985032555, 0.9057369185712403, 0.7056790022649976, 0.9467768652420852, 0.525807541471704, 1.0, 0.39026506625768903, 0.6736694802706051, 0.49409920192560297, 0.7824388143176209, 0.30520251256162473, 0.8302748506216027, 0.18640122764291142, 0.8517760982367321, 0.08245851646205951, 0.4501800703393166, 0.4683188782936545, 0.5275061723993476, 0.25533928220639646, 0.5631709830850691, 0.11651409884099338, 0.5807576887009087, 0.0, 0.22954898677889182, 0.47779349706639024, 0.2837783762660866, 0.2709084782387454, 0.34145061118833286, 0.1424759240456597, 0.3891468507979905, 0.034684551154154745, 0.0, 0.5197089702899528, 0.013250193393245756, 0.36608171978878956, 0.03225714898246302, 0.2606386251914124, 0.05730163294735998, 0.16211389127538064] (of type <class 'list'>)"
     ]
    }
   ],
   "source": [
    "# import cv2\n",
    "# import numpy as np\n",
    "# import mediapipe as mp\n",
    "\n",
    "# # Label dictionary for character predictions\n",
    "# labels_dict = {0: 'අ', 1: 'ආ', 2: 'ඇ'}\n",
    "\n",
    "# # Initialize video capture\n",
    "# cap = cv2.VideoCapture(0)\n",
    "\n",
    "# # Initialize Mediapipe Hands\n",
    "# mp_hands = mp.solutions.hands\n",
    "# hands = mp_hands.Hands()\n",
    "# mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# # Assuming `model` is already trained and loaded\n",
    "# while True:\n",
    "#     data_aux = []\n",
    "#     x_ = []\n",
    "#     y_ = []\n",
    "\n",
    "#     ret, frame = cap.read()\n",
    "#     if not ret:\n",
    "#         break\n",
    "\n",
    "#     H, W, _ = frame.shape\n",
    "#     frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "#     results = hands.process(frame_rgb)\n",
    "\n",
    "#     if results.multi_hand_landmarks:\n",
    "#         for hand_landmarks in results.multi_hand_landmarks:\n",
    "#             mp_drawing.draw_landmarks(\n",
    "#                 frame,\n",
    "#                 hand_landmarks,\n",
    "#                 mp_hands.HAND_CONNECTIONS,\n",
    "#                 mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "#                 mp_drawing_styles.get_default_hand_connections_style()\n",
    "#             )\n",
    "\n",
    "#             for landmark in hand_landmarks.landmark:\n",
    "#                 x_.append(landmark.x)\n",
    "#                 y_.append(landmark.y)\n",
    "\n",
    "#             # Use only the first 21 landmarks (assuming 21 is half of 42 features)\n",
    "#             for i in range(21):  # 21 landmarks for 42 features (x and y)\n",
    "#                 data_aux.append((x_[i] - min(x_)) / (max(x_) - min(x_)))\n",
    "#                 data_aux.append((y_[i] - min(y_)) / (max(y_) - min(y_)))\n",
    "\n",
    "#         x1 = max(0, int(min(x_) * W) - 10)\n",
    "#         y1 = max(0, int(min(y_) * H) - 10)\n",
    "#         x2 = min(W, int(max(x_) * W) + 10)\n",
    "#         y2 = min(H, int(max(y_) * H) + 10)\n",
    "\n",
    "#         # Check if we have the right number of features\n",
    "#         print(f\"Feature count: {len(data_aux)}\")\n",
    "\n",
    "#         if len(data_aux) == 42:  # Ensure exactly 42 features\n",
    "#             data_aux = np.asarray(data_aux).reshape(1, -1)\n",
    "#             prediction = model.predict(data_aux)\n",
    "#             predicted_character = labels_dict[int(prediction[0])]\n",
    "\n",
    "#             cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 0), 4)\n",
    "#             cv2.putText(frame, predicted_character, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 1.3, (0, 0, 0), 3, cv2.LINE_AA)\n",
    "#         else:\n",
    "#             print(\"Not enough features collected.\")\n",
    "\n",
    "#     cv2.imshow('frame', frame)\n",
    "\n",
    "#     if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#         break\n",
    "\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a67bff2-e318-4a45-b720-fb759f4b0d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n",
      "Feature capture started.\n",
      "Feature count: 42\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 246ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "can only convert an array of size 1 to a Python scalar",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 79\u001b[0m\n\u001b[0;32m     76\u001b[0m prediction \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(data_aux)\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# Convert the prediction to a scalar and map it to the corresponding character\u001b[39;00m\n\u001b[1;32m---> 79\u001b[0m predicted_character \u001b[38;5;241m=\u001b[39m labels_dict[\u001b[38;5;28mint\u001b[39m(prediction\u001b[38;5;241m.\u001b[39mitem())]\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# Draw the bounding box around the detected hand\u001b[39;00m\n\u001b[0;32m     82\u001b[0m x1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mmin\u001b[39m(x_) \u001b[38;5;241m*\u001b[39m W) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m10\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: can only convert an array of size 1 to a Python scalar"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import pickle  # For loading the model\n",
    "\n",
    "# Label dictionary for character predictions\n",
    "labels_dict = {0: 'අ', 1: 'ආ', 2: 'ඇ'}\n",
    "\n",
    "# Load the trained model (ensure the model is trained and saved properly)\n",
    "try:\n",
    "    with open('model.p', 'rb') as f:\n",
    "        model = pickle.load(f)['model']\n",
    "        print(\"Model loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    model = None\n",
    "\n",
    "# Initialize video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Initialize Mediapipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands()\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "capture_started = False  # Flag to start capturing features\n",
    "\n",
    "while True:\n",
    "    data_aux = []\n",
    "    x_ = []\n",
    "    y_ = []\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to capture frame.\")\n",
    "        break\n",
    "\n",
    "    H, W, _ = frame.shape\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(frame_rgb)\n",
    "\n",
    "    # Display message to user\n",
    "    if not capture_started:\n",
    "        cv2.putText(frame, \"Press 's' to start capturing hand features\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    \n",
    "    # If capture has started, detect hand landmarks\n",
    "    if capture_started:\n",
    "        # If hands are detected\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                # Draw hand landmarks on the frame\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    frame,\n",
    "                    hand_landmarks,\n",
    "                    mp_hands.HAND_CONNECTIONS\n",
    "                )\n",
    "\n",
    "                # Collect x and y coordinates for the landmarks\n",
    "                for landmark in hand_landmarks.landmark:\n",
    "                    x_.append(landmark.x)\n",
    "                    y_.append(landmark.y)\n",
    "\n",
    "                # Use only the first 21 landmarks (each having x and y coordinates) for 42 features\n",
    "                for i in range(21):  # Only 21 landmarks (each having x and y)\n",
    "                    data_aux.append((x_[i] - min(x_)) / (max(x_) - min(x_)))\n",
    "                    data_aux.append((y_[i] - min(y_)) / (max(y_) - min(y_)))\n",
    "\n",
    "            # Ensure exactly 42 features\n",
    "            print(f\"Feature count: {len(data_aux)}\")\n",
    "\n",
    "            # Convert the list to a NumPy array and reshape to match the model's input\n",
    "            data_aux = np.array(data_aux).reshape(1, -1)  # Reshape to (1, 42)\n",
    "\n",
    "            # Ensure that model is loaded before making predictions\n",
    "            if model:\n",
    "                prediction = model.predict(data_aux)\n",
    "                \n",
    "                # Convert the prediction to a scalar and map it to the corresponding character\n",
    "                predicted_character = labels_dict[int(prediction.item())]\n",
    "\n",
    "                # Draw the bounding box around the detected hand\n",
    "                x1 = max(0, int(min(x_) * W) - 10)\n",
    "                y1 = max(0, int(min(y_) * H) - 10)\n",
    "                x2 = min(W, int(max(x_) * W) + 10)\n",
    "                y2 = min(H, int(max(y_) * H) + 10)\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 0), 4)\n",
    "                cv2.putText(frame, predicted_character, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 1.3, (0, 0, 0), 3, cv2.LINE_AA)\n",
    "            else:\n",
    "                print(\"Model not loaded. Cannot make prediction.\")\n",
    "    \n",
    "    # Display the frame\n",
    "    cv2.imshow('frame', frame)\n",
    "\n",
    "    # Wait for key press to either start capture or exit\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord('s'):  # Start capturing features when 's' is pressed\n",
    "        capture_started = True\n",
    "        print(\"Feature capture started.\")\n",
    "    elif key == ord('q'):  # Quit the loop when 'q' is pressed\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "198df1f1-b469-4be1-b56d-9d70562c699f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n",
      "Feature capture started.\n",
      "Feature count: 42\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 325ms/step\n",
      "Feature count: 42\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acer\\AppData\\Local\\Temp\\ipykernel_14812\\2052500042.py:79: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  predicted_class = int(np.argmax(prediction, axis=-1))  # Get the index of the max value if it's a multi-class output\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Feature count: 42\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Feature count: 42\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Feature count: 42\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Feature count: 42\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "Feature count: 42\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Feature count: 42\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Feature count: 42\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Feature count: 42\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Feature count: 42\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Feature count: 42\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Feature count: 42\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Feature count: 42\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Feature count: 42\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Feature count: 42\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Feature count: 42\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Feature count: 42\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Feature count: 42\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Feature count: 42\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "Feature count: 42\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Feature count: 42\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Feature count: 42\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Feature count: 42\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "Feature count: 42\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Feature count: 42\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Feature count: 42\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Feature count: 42\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Feature count: 42\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "Feature count: 42\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Feature count: 42\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Feature count: 42\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Feature count: 42\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "Feature count: 42\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Feature count: 42\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "Feature count: 42\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Feature count: 42\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "Feature count: 42\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
      "Feature count: 42\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Feature count: 42\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "Feature count: 42\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "Feature count: 42\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "Feature count: 42\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Feature count: 42\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Feature count: 42\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Feature count: 42\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Feature count: 42\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Feature count: 42\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Feature count: 42\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Feature count: 42\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step\n",
      "Feature count: 42\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "Feature count: 42\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Feature count: 42\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Feature count: 42\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "Feature count: 42\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Feature count: 42\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "Feature count: 42\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Feature count: 42\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "Feature count: 42\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "Feature count: 42\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Feature count: 42\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Feature count: 42\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "Feature count: 84\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node sequential_1/dense_1/Relu defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"C:\\Users\\acer\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n\n  File \"C:\\Users\\acer\\anaconda3\\Lib\\site-packages\\traitlets\\config\\application.py\", line 992, in launch_instance\n\n  File \"C:\\Users\\acer\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 701, in start\n\n  File \"C:\\Users\\acer\\anaconda3\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n\n  File \"C:\\Users\\acer\\anaconda3\\Lib\\asyncio\\windows_events.py\", line 321, in run_forever\n\n  File \"C:\\Users\\acer\\anaconda3\\Lib\\asyncio\\base_events.py\", line 607, in run_forever\n\n  File \"C:\\Users\\acer\\anaconda3\\Lib\\asyncio\\base_events.py\", line 1922, in _run_once\n\n  File \"C:\\Users\\acer\\anaconda3\\Lib\\asyncio\\events.py\", line 80, in _run\n\n  File \"C:\\Users\\acer\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in dispatch_queue\n\n  File \"C:\\Users\\acer\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 523, in process_one\n\n  File \"C:\\Users\\acer\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 429, in dispatch_shell\n\n  File \"C:\\Users\\acer\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n\n  File \"C:\\Users\\acer\\anaconda3\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 429, in do_execute\n\n  File \"C:\\Users\\acer\\anaconda3\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n\n  File \"C:\\Users\\acer\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3051, in run_cell\n\n  File \"C:\\Users\\acer\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3106, in _run_cell\n\n  File \"C:\\Users\\acer\\anaconda3\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"C:\\Users\\acer\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3311, in run_cell_async\n\n  File \"C:\\Users\\acer\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3493, in run_ast_nodes\n\n  File \"C:\\Users\\acer\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3553, in run_code\n\n  File \"C:\\Users\\acer\\AppData\\Local\\Temp\\ipykernel_14812\\2052500042.py\", line 76, in <module>\n\n  File \"C:\\Users\\acer\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"C:\\Users\\acer\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 504, in predict\n\n  File \"C:\\Users\\acer\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 204, in one_step_on_data_distributed\n\n  File \"C:\\Users\\acer\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 194, in one_step_on_data\n\n  File \"C:\\Users\\acer\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 92, in predict_step\n\n  File \"C:\\Users\\acer\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"C:\\Users\\acer\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\layer.py\", line 846, in __call__\n\n  File \"C:\\Users\\acer\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"C:\\Users\\acer\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\ops\\operation.py\", line 48, in __call__\n\n  File \"C:\\Users\\acer\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 156, in error_handler\n\n  File \"C:\\Users\\acer\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\models\\sequential.py\", line 209, in call\n\n  File \"C:\\Users\\acer\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\models\\functional.py\", line 202, in call\n\n  File \"C:\\Users\\acer\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\ops\\function.py\", line 155, in _run_through_graph\n\n  File \"C:\\Users\\acer\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\models\\functional.py\", line 592, in call\n\n  File \"C:\\Users\\acer\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"C:\\Users\\acer\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\layer.py\", line 846, in __call__\n\n  File \"C:\\Users\\acer\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"C:\\Users\\acer\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\ops\\operation.py\", line 48, in __call__\n\n  File \"C:\\Users\\acer\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 156, in error_handler\n\n  File \"C:\\Users\\acer\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\core\\dense.py\", line 154, in call\n\n  File \"C:\\Users\\acer\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\activations\\activations.py\", line 47, in relu\n\n  File \"C:\\Users\\acer\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\activations\\activations.py\", line 99, in static_call\n\n  File \"C:\\Users\\acer\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\backend\\tensorflow\\nn.py\", line 15, in relu\n\nMatrix size-incompatible: In[0]: [1,2432], In[1]: [1152,128]\n\t [[{{node sequential_1/dense_1/Relu}}]] [Op:__inference_one_step_on_data_distributed_3595]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 76\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# Ensure that model is loaded before making predictions\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model:\n\u001b[1;32m---> 76\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(data_aux)\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;66;03m# Access the first prediction value if it's a 1D array\u001b[39;00m\n\u001b[0;32m     79\u001b[0m     predicted_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(np\u001b[38;5;241m.\u001b[39margmax(prediction, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))  \u001b[38;5;66;03m# Get the index of the max value if it's a multi-class output\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node sequential_1/dense_1/Relu defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"C:\\Users\\acer\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n\n  File \"C:\\Users\\acer\\anaconda3\\Lib\\site-packages\\traitlets\\config\\application.py\", line 992, in launch_instance\n\n  File \"C:\\Users\\acer\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 701, in start\n\n  File \"C:\\Users\\acer\\anaconda3\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n\n  File \"C:\\Users\\acer\\anaconda3\\Lib\\asyncio\\windows_events.py\", line 321, in run_forever\n\n  File \"C:\\Users\\acer\\anaconda3\\Lib\\asyncio\\base_events.py\", line 607, in run_forever\n\n  File \"C:\\Users\\acer\\anaconda3\\Lib\\asyncio\\base_events.py\", line 1922, in _run_once\n\n  File \"C:\\Users\\acer\\anaconda3\\Lib\\asyncio\\events.py\", line 80, in _run\n\n  File \"C:\\Users\\acer\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in dispatch_queue\n\n  File \"C:\\Users\\acer\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 523, in process_one\n\n  File \"C:\\Users\\acer\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 429, in dispatch_shell\n\n  File \"C:\\Users\\acer\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n\n  File \"C:\\Users\\acer\\anaconda3\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 429, in do_execute\n\n  File \"C:\\Users\\acer\\anaconda3\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n\n  File \"C:\\Users\\acer\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3051, in run_cell\n\n  File \"C:\\Users\\acer\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3106, in _run_cell\n\n  File \"C:\\Users\\acer\\anaconda3\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n\n  File \"C:\\Users\\acer\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3311, in run_cell_async\n\n  File \"C:\\Users\\acer\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3493, in run_ast_nodes\n\n  File \"C:\\Users\\acer\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3553, in run_code\n\n  File \"C:\\Users\\acer\\AppData\\Local\\Temp\\ipykernel_14812\\2052500042.py\", line 76, in <module>\n\n  File \"C:\\Users\\acer\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"C:\\Users\\acer\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 504, in predict\n\n  File \"C:\\Users\\acer\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 204, in one_step_on_data_distributed\n\n  File \"C:\\Users\\acer\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 194, in one_step_on_data\n\n  File \"C:\\Users\\acer\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py\", line 92, in predict_step\n\n  File \"C:\\Users\\acer\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"C:\\Users\\acer\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\layer.py\", line 846, in __call__\n\n  File \"C:\\Users\\acer\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"C:\\Users\\acer\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\ops\\operation.py\", line 48, in __call__\n\n  File \"C:\\Users\\acer\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 156, in error_handler\n\n  File \"C:\\Users\\acer\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\models\\sequential.py\", line 209, in call\n\n  File \"C:\\Users\\acer\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\models\\functional.py\", line 202, in call\n\n  File \"C:\\Users\\acer\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\ops\\function.py\", line 155, in _run_through_graph\n\n  File \"C:\\Users\\acer\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\models\\functional.py\", line 592, in call\n\n  File \"C:\\Users\\acer\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"C:\\Users\\acer\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\layer.py\", line 846, in __call__\n\n  File \"C:\\Users\\acer\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 117, in error_handler\n\n  File \"C:\\Users\\acer\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\ops\\operation.py\", line 48, in __call__\n\n  File \"C:\\Users\\acer\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 156, in error_handler\n\n  File \"C:\\Users\\acer\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\core\\dense.py\", line 154, in call\n\n  File \"C:\\Users\\acer\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\activations\\activations.py\", line 47, in relu\n\n  File \"C:\\Users\\acer\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\activations\\activations.py\", line 99, in static_call\n\n  File \"C:\\Users\\acer\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\backend\\tensorflow\\nn.py\", line 15, in relu\n\nMatrix size-incompatible: In[0]: [1,2432], In[1]: [1152,128]\n\t [[{{node sequential_1/dense_1/Relu}}]] [Op:__inference_one_step_on_data_distributed_3595]"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import pickle  # For loading the model\n",
    "\n",
    "# Label dictionary for character predictions\n",
    "labels_dict = {0: 'අ', 1: 'ආ', 2: 'ඇ'}\n",
    "\n",
    "# Load the trained model (ensure the model is trained and saved properly)\n",
    "try:\n",
    "    with open('model.p', 'rb') as f:\n",
    "        model = pickle.load(f)['model']\n",
    "        print(\"Model loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    model = None\n",
    "\n",
    "# Initialize video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Initialize Mediapipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands()\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "capture_started = False  # Flag to start capturing features\n",
    "\n",
    "while True:\n",
    "    data_aux = []\n",
    "    x_ = []\n",
    "    y_ = []\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to capture frame.\")\n",
    "        break\n",
    "\n",
    "    H, W, _ = frame.shape\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(frame_rgb)\n",
    "\n",
    "    # Display message to user\n",
    "    if not capture_started:\n",
    "        cv2.putText(frame, \"Press 's' to start capturing hand features\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    \n",
    "    # If capture has started, detect hand landmarks\n",
    "    if capture_started:\n",
    "        # If hands are detected\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                # Draw hand landmarks on the frame\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    frame,\n",
    "                    hand_landmarks,\n",
    "                    mp_hands.HAND_CONNECTIONS\n",
    "                )\n",
    "\n",
    "                # Collect x and y coordinates for the landmarks\n",
    "                for landmark in hand_landmarks.landmark:\n",
    "                    x_.append(landmark.x)\n",
    "                    y_.append(landmark.y)\n",
    "\n",
    "                # Use only the first 21 landmarks (each having x and y coordinates) for 42 features\n",
    "                for i in range(21):  # Only 21 landmarks (each having x and y)\n",
    "                    data_aux.append((x_[i] - min(x_)) / (max(x_) - min(x_)))\n",
    "                    data_aux.append((y_[i] - min(y_)) / (max(y_) - min(y_)))\n",
    "\n",
    "            # Ensure exactly 42 features\n",
    "            print(f\"Feature count: {len(data_aux)}\")\n",
    "\n",
    "            # Convert the list to a NumPy array and reshape to match the model's input\n",
    "            data_aux = np.array(data_aux).reshape(1, -1)  # Reshape to (1, 42)\n",
    "\n",
    "            # Ensure that model is loaded before making predictions\n",
    "            if model:\n",
    "                prediction = model.predict(data_aux)\n",
    "\n",
    "                # Access the first prediction value if it's a 1D array\n",
    "                predicted_class = int(np.argmax(prediction, axis=-1))  # Get the index of the max value if it's a multi-class output\n",
    "                predicted_character = labels_dict[predicted_class]\n",
    "\n",
    "                # Draw the bounding box around the detected hand\n",
    "                x1 = max(0, int(min(x_) * W) - 10)\n",
    "                y1 = max(0, int(min(y_) * H) - 10)\n",
    "                x2 = min(W, int(max(x_) * W) + 10)\n",
    "                y2 = min(H, int(max(y_) * H) + 10)\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 0), 4)\n",
    "                cv2.putText(frame, predicted_character, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 1.3, (0, 0, 0), 3, cv2.LINE_AA)\n",
    "            else:\n",
    "                print(\"Model not loaded. Cannot make prediction.\")\n",
    "    \n",
    "    # Display the frame\n",
    "    cv2.imshow('frame', frame)\n",
    "\n",
    "    # Wait for key press to either start capture or exit\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord('s'):  # Start capturing features when 's' is pressed\n",
    "        capture_started = True\n",
    "        print(\"Feature capture started.\")\n",
    "    elif key == ord('q'):  # Quit the loop when 'q' is pressed\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61d0a15-a71a-4a5d-a168-fb1540117df1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
